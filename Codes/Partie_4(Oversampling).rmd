---
title: "Oversampling"
author: "Loïc LAFFINEUR"
date: "`r Sys.Date()`"
output: html_document
---

```{r library, include=FALSE}
#Manipulation : 
library(dplyr)

#Modèles de ML :
library(randomForest) #RF ;
library(rpart) #CART ;
library(xgboost) #XGBoost ;
library(glmnet)
library(class)
library(naivebayes)
library(ranger)
library(kernlab)
library(e1071) #Algo de ML

#Autres pour ML :
library(caret) #Split/Search/... ;
library(caretEnsemble)

library(MLmetrics) #Metriques d'évaluation
library(ROSE)
```

```{r, mat_model_names}
load("intro_data.RData")
rm(tab_ML)
model_names=c("Regression logistique","K_NN","Naive Bayes","SVM RBF","SVM Polynomial","CART","Ranger","RandomForest","XGBoost")
```

# Création de la table : 

```{r, oversampling}
#oversampling et Oversampling : Je ne modifie QUE la composition du TRAIN !
set.seed(12)

over_train <- upSample(x = data_train[, -ncol(data_train)],
                         y = data_train$top_sanction)
colnames(over_train)[ncol(over_train)]=c("top_sanction")
table(over_train$top_sanction)

over_acp <- upSample(x = acp_train[, -ncol(acp_train)],
                         y = acp_train$top_sanction)
colnames(over_acp)[ncol(over_acp)]=c("top_sanction")
```

```{r, procedure pour CV}
over_train$nb_phies=as.numeric(over_train$nb_phies)
over_train$active_tout2018_1=as.numeric(over_train$active_tout2018_1)-1
over_train$top_sanction=as.factor(make.names(over_train$top_sanction))

over_acp$top_sanction=as.factor(make.names(over_acp$top_sanction))

X_test$active_tout2018_1=as.numeric(X_test$active_tout2018_1)-1
y_test=as.factor(make.names(y_test))

#Création de la base de donnée en bonne forme : 
#Train : 
x_over=data.matrix(over_train[,-ncol(over_train)])
x_over_acp=data.matrix(over_acp[,-ncol(over_acp)])

#Test : 
test_data=data.matrix(X_test); test_data[,1]=test_data[,1]-1
test_acp=data.matrix(acp_test)

#Nos folds pour la CV :
set.seed(12)
folds_data=createFolds(over_train$top_sanction,k=5)
folds_acp=createFolds(over_acp$top_sanction,k=5)

#Règle de CV : 
Control_data <- trainControl(method = "cv", number = 5, index=folds_data, search='random',
                           classProbs = T, #Pour mes metrics AUC Sensi et Spé
                           summaryFunction = twoClassSummary, verboseIter = F)

Control_acp <- trainControl(method = "cv", number = 5, index=folds_acp, search='random',
                           classProbs = T, #Pour mes metrics AUC Sensi et Spé
                           summaryFunction = twoClassSummary, verboseIter = F)
```

```{r, entrainement des modèles avec CV,warning=F}
set.seed(12)
#Liste des modèles :
start_time=Sys.time()
model_list_over <- caretList(trControl = Control_data, 
                        tuneList = list(
                          glm=caretModelSpec(x=x_over, y=over_train$top_sanction,
                                             method="glmnet", tuneLength=300,
                                             metric="ROC"),
                          knn=caretModelSpec(x=x_over, y=over_train$top_sanction,
                                             method="knn", tuneLength=300,
                                             metric="ROC"), 
                          naive_bayes=caretModelSpec(x=x_over, y=over_train$top_sanction,
                                             method="naive_bayes", tuneLength=300,
                                             metric="ROC"),
                          svm_rad=caretModelSpec(x=x_over, y=over_train$top_sanction,
                                             method="svmRadial", tuneLength=300,
                                             metric="ROC"),
                          svm_poly=caretModelSpec(x=x_over, y=over_train$top_sanction,
                                             method="svmPoly", tuneLength=300,
                                             metric="ROC"),
                          rpart=caretModelSpec(x=x_over, y=over_train$top_sanction,
                                             method="rpart2", tuneLength=300,
                                             metric="ROC"),
                          ranger=caretModelSpec(x=x_over, y=over_train$top_sanction,
                                             method="ranger", tuneLength=500,
                                             metric="ROC",verbose=F),
                          rf=caretModelSpec(x=x_over, y=over_train$top_sanction,
                                             method="rf", tuneLength=500,
                                             metric="ROC"),
                          xgbTree=caretModelSpec(x=x_over, y=over_train$top_sanction,
                                             method="xgbTree", tuneLength=500,
                                             metric="ROC",verbosity=0)
                          ),
                        continue_on_fail = FALSE, preProcess = c("center", "scale","nzv"))
end_time=Sys.time()
end_time-start_time #Time difference of 48.18866 mins


#Liste des modèles :
set.seed(12)
start_time=Sys.time()
model_list_over_acp <- caretList(trControl = Control_acp, 
                        tuneList = list(
                          glm=caretModelSpec(x=x_over_acp, y=over_acp$top_sanction,
                                             method="glmnet", tuneLength=300,
                                             metric="ROC"),
                          knn=caretModelSpec(x=x_over_acp, y=over_acp$top_sanction,
                                             method="knn", tuneLength=300,
                                             metric="ROC"), 
                          naive_bayes=caretModelSpec(x=x_over_acp, y=over_acp$top_sanction,
                                             method="naive_bayes", tuneLength=300,
                                             metric="ROC"),
                          svm_rad=caretModelSpec(x=x_over_acp, y=over_acp$top_sanction,
                                             method="svmRadial", tuneLength=300,
                                             metric="ROC"),
                          svm_poly=caretModelSpec(x=x_over_acp, y=over_acp$top_sanction,
                                             method="svmPoly", tuneLength=300,
                                             metric="ROC"),
                          rpart=caretModelSpec(x=x_over_acp, y=over_acp$top_sanction,
                                             method="rpart2", tuneLength=300,
                                             metric="ROC"),
                          ranger=caretModelSpec(x=x_over_acp, y=over_acp$top_sanction,
                                             method="ranger", tuneLength=500,
                                             metric="ROC",verbose=F),
                          rf=caretModelSpec(x=x_over_acp, y=over_acp$top_sanction,
                                             method="rf", tuneLength=500,
                                             metric="ROC"),
                          xgbTree=caretModelSpec(x=x_over_acp, y=over_acp$top_sanction,
                                             method="xgbTree", tuneLength=500,
                                             metric="ROC")
                          ),
                        continue_on_fail = FALSE, preProcess = c("center", "scale","nzv"))
end_time=Sys.time()
end_time-start_time #Time difference of 40.66627 mins

rm(start_time,end_time)
```

# Régression Logistique :

```{r}
Erreur(predict(model_list_over$glm,x_over),over_train$top_sanction)
Erreur(predict(model_list_over$glm,test_data),y_test)
Erreur(predict(model_list_over_acp$glm,x_over_acp),over_acp$top_sanction)
Erreur(predict(model_list_over_acp$glm,test_acp),y_test)
```

Sous-apprentissage et faible Precision.

```{r, etude des résultats}
col=rainbow(4)
roc.curve(over_train$top_sanction,predict(model_list_over$glm,x_over),col=col[1],main="ROC Curve régression logistique")
roc.curve(y_test,predict(model_list_over$glm,test_data),add.roc = T,col=col[2])
roc.curve(over_acp$top_sanction,predict(model_list_over_acp$glm,x_over_acp),add.roc = T,col=col[3])
roc.curve(y_test,predict(model_list_over_acp$glm,test_acp),add.roc = T,col=col[4])
legend('bottomright',legend=c("Train sans ACP","Test sans ACP","Train avec ACP","Test avec ACP"),lwd=2,col=col)
rm(col)
```

```{r}
### Importance des variables : 
barplot(as.matrix(varImp(model_list_over$glm)$importance)[,1],las=2,col=rainbow(ncol(over_train)),main="Importance des variables (Sans ACP)")
barplot(as.matrix(varImp(model_list_over_acp$glm)$importance)[,1],las=2,col=rainbow(ncol(over_acp)),main="Importance des variables (Avec ACP)")
```

# k-NN :

```{r}
Erreur(predict(model_list_over$knn,x_over),over_train$top_sanction)
Erreur(predict(model_list_over$knn,test_data),y_test)
Erreur(predict(model_list_over_acp$knn,x_over_acp),over_acp$top_sanction)
Erreur(predict(model_list_over_acp$knn,test_acp),y_test)
```

Le premier K-nn sur-apprend et se généralise très mal sur les individus du test.
Le deuxieme sous-apprend et est tout aussi mauvais sur le test.

```{r, etude des résultats}
col=rainbow(4)
roc.curve(over_train$top_sanction,predict(model_list_over$knn,x_over),col=col[1],main="ROC Curve k-NN")
roc.curve(y_test,predict(model_list_over$knn,test_data),add.roc = T,col=col[2])
roc.curve(over_acp$top_sanction,predict(model_list_over_acp$knn,x_over_acp),add.roc = T,col=col[3])
roc.curve(y_test,predict(model_list_over_acp$knn,test_acp),add.roc = T,col=col[4])
legend('bottomright',legend=c("Train sans ACP","Test sans ACP","Train avec ACP","Test avec ACP"),lwd=2,col=col)
rm(col)
```

```{r, importance}
# Importance des variables :
barplot(as.matrix(varImp(model_list_over$knn)$importance)[,1],las=2,col=rainbow(ncol(over_train)),main="Importance des variables (Sans ACP)")

barplot(as.matrix(varImp(model_list_over_acp$knn)$importance)[,1],las=2,col=rainbow(ncol(over_acp)),main="Importance des variables (Avec ACP)")
```

# Naive Bayes :

```{r}
Erreur(predict(model_list_over$naive_bayes,x_over),over_train$top_sanction)
Erreur(predict(model_list_over$naive_bayes,test_data),y_test)
Erreur(predict(model_list_over_acp$naive_bayes,x_over_acp),over_acp$top_sanction)
Erreur(predict(model_list_over_acp$naive_bayes,test_acp),y_test)
```

Sous-apprentissage et faible precision sur la classe X0.

```{r, etude des résultats}
col=rainbow(4)
roc.curve(over_train$top_sanction,predict(model_list_over$naive_bayes,x_over),col=col[1],main="ROC Curve Naive Bayes")
roc.curve(y_test,predict(model_list_over$naive_bayes,test_data),add.roc = T,col=col[2])
roc.curve(over_acp$top_sanction,predict(model_list_over_acp$naive_bayes,x_over_acp),add.roc = T,col=col[3])
roc.curve(y_test,predict(model_list_over_acp$naive_bayes,test_acp),add.roc = T,col=col[4])
legend('bottomright',legend=c("Train sans ACP","Test sans ACP","Train avec ACP","Test avec ACP"),lwd=2,col=col)
rm(col)
```

```{r, importance}
#Importance des variables
barplot(as.matrix(varImp(model_list_over$naive_bayes)$importance)[,1],las=2,col=rainbow(ncol(over_train)),main="Importance des variables (Sans ACP)")
barplot(as.matrix(varImp(model_list_over_acp$naive_bayes)$importance)[,1],las=2,col=rainbow(ncol(over_acp)),main="Importance des variables (Avec ACP)")
```

# SVM : 

## RBF : 

```{r}
Erreur(predict(model_list_over$svm_rad,x_over),over_train$top_sanction)
Erreur(predict(model_list_over$svm_rad,test_data),y_test)
Erreur(predict(model_list_over_acp$svm_rad,x_over_acp),over_acp$top_sanction)
Erreur(predict(model_list_over_acp$svm_rad,test_acp),y_test)
```

Sur-apprentissage, notre modèle sans ACP arrive quand même à deceler un peu des caractéristiques de nos classes, il n'est pas parfait mais il n'est pas non plus dans un cas de sous-apprentissage car il a quand même de bons résultats. Les résultats en test sont inférieurs mais reste correctes par rapport à ce qu'on a eu avant.

```{r, etude des résultats}
col=rainbow(4)
roc.curve(over_train$top_sanction,predict(model_list_over$svm_rad,x_over),col=col[1],main="ROC Curve SVM avec noyau RBF")
roc.curve(y_test,predict(model_list_over$svm_rad,test_data),add.roc = T,col=col[2])
roc.curve(over_acp$top_sanction,predict(model_list_over_acp$svm_rad,x_over_acp),add.roc = T,col=col[3])
roc.curve(y_test,predict(model_list_over_acp$svm_rad,test_acp),add.roc = T,col=col[4])
legend('bottomright',legend=c("Train sans ACP","Test sans ACP","Train avec ACP","Test avec ACP"),lwd=2,col=col)
rm(col)
```

```{r, importance}
#Importance des variables
barplot(as.matrix(varImp(model_list_over$svm_rad)$importance)[,1],las=2,col=rainbow(ncol(over_train)),main="Importance des variables (Sans ACP)")

barplot(as.matrix(varImp(model_list_over_acp$svm_rad)$importance)[,1],las=2,col=rainbow(ncol(over_acp)),main="Importance des variables (Avec ACP)")
```

## Polynomial : 

```{r}
Erreur(predict(model_list_over$svm_poly,x_over),over_train$top_sanction)
Erreur(predict(model_list_over$svm_poly,test_data),y_test)
Erreur(predict(model_list_over_acp$svm_poly,x_over_acp),over_acp$top_sanction)
Erreur(predict(model_list_over_acp$svm_poly,test_acp),y_test)
```

Les résultats pour le SVM Polynomiaux sont légérement inférieurs à ceux du RBF mais les conclusions sont les mêmes.

```{r, etude des résultats}
col=rainbow(4)
roc.curve(over_train$top_sanction,predict(model_list_over$svm_poly,x_over),col=col[1],main="ROC Curve SVM avec noyau Polynomial")
roc.curve(y_test,predict(model_list_over$svm_poly,test_data),add.roc = T,col=col[2])
roc.curve(over_acp$top_sanction,predict(model_list_over_acp$svm_poly,x_over_acp),add.roc = T,col=col[3])
roc.curve(y_test,predict(model_list_over_acp$svm_poly,test_acp),add.roc = T,col=col[4])
legend('bottomright',legend=c("Train sans ACP","Test sans ACP","Train avec ACP","Test avec ACP"),lwd=2,col=col)
rm(col)
```

```{r, importance}
#Importance des variables
barplot(as.matrix(varImp(model_list_over$svm_poly)$importance)[,1],las=2,col=rainbow(ncol(over_train)),main="Importance des variables (Sans ACP)")
barplot(as.matrix(varImp(model_list_over_acp$svm_poly)$importance)[,1],las=2,col=rainbow(ncol(over_acp)),main="Importance des variables (Avec ACP)")
```

# Decision Tree :

```{r}
Erreur(predict(model_list_over$rpart,x_over),over_train$top_sanction)
Erreur(predict(model_list_over$rpart,test_data),y_test)
Erreur(predict(model_list_over_acp$rpart,x_over_acp),over_acp$top_sanction)
Erreur(predict(model_list_over_acp$rpart,test_acp),y_test)
```

Nos deux CART sur-apprennent, On a une faible précision, un mauvais AUC en test.

```{r, etude des résultats}
col=rainbow(4)
roc.curve(over_train$top_sanction,predict(model_list_over$rpart,x_over),col=col[1],main="ROC Curve CART")
roc.curve(y_test,predict(model_list_over$rpart,test_data),add.roc = T,col=col[2])
roc.curve(over_acp$top_sanction,predict(model_list_over_acp$rpart,x_over_acp),add.roc = T,col=col[3])
roc.curve(y_test,predict(model_list_over_acp$rpart,test_acp),add.roc = T,col=col[4])
legend('bottomright',legend=c("Train sans ACP","Test sans ACP","Train avec ACP","Test avec ACP"),lwd=2,col=col)
rm(col)
```

```{r, plot_tree, fig.height=8}
plot(model_list_over$rpart$finalModel,uniform=T)
text(model_list_over$rpart$finalModel)

plot(model_list_over_acp$rpart$finalModel,uniform=T)
text(model_list_over_acp$rpart$finalModel)
```

```{r, importance_tree}
barplot(as.matrix(varImp(model_list_over$rpart)$importance)[,1],las=2,col=rainbow(ncol(over_train)),main="Importance des variables (Sans ACP)")

barplot(as.matrix(varImp(model_list_over_acp$rpart)$importance)[,1],las=2,col=rainbow(ncol(over_acp)),main="Importance des variables (Avec ACP)")
```

# Random Forest : 

## randomForest : 

```{r}
Erreur(predict(model_list_over$rf,x_over),over_train$top_sanction)
Erreur(predict(model_list_over$rf,test_data),y_test)
Erreur(predict(model_list_over_acp$rf,x_over_acp),over_acp$top_sanction)
Erreur(predict(model_list_over_acp$rf,test_acp),y_test)
```

Sur-apprentissage, AUC faible, faible précision, faible F1-score.

```{r, etude des résultats}
col=rainbow(4)
roc.curve(over_train$top_sanction,predict(model_list_over$rf,x_over),col=col[1],main="ROC Curve avec randomForest")
roc.curve(y_test,predict(model_list_over$rf,test_data),add.roc = T,col=col[2])
roc.curve(over_acp$top_sanction,predict(model_list_over_acp$rf,x_over_acp),add.roc = T,col=col[3])
roc.curve(y_test,predict(model_list_over_acp$rf,test_acp),add.roc = T,col=col[4])
legend('bottomright',legend=c("Train sans ACP","Test sans ACP","Train avec ACP","Test avec ACP"),lwd=2,col=col)
rm(col)
```

```{r, importance}
#Importance des variables
barplot(as.matrix(varImp(model_list_over$rf)$importance)[,1],las=2,col=rainbow(ncol(over_train)),main="Importance des variables (Sans ACP)")

barplot(as.matrix(varImp(model_list_over_acp$rf)$importance)[,1],las=2,col=rainbow(ncol(over_acp)),main="Importance des variables (Avec ACP)")
```

## Ranger : 

```{r}
Erreur(predict(model_list_over$ranger,x_over),over_train$top_sanction)
Erreur(predict(model_list_over$ranger,test_data),y_test)
Erreur(predict(model_list_over_acp$ranger,x_over_acp),over_acp$top_sanction)
Erreur(predict(model_list_over_acp$ranger,test_acp),y_test)
```

Sur-apprentissage mais de meilleur résultats que sur le package randomForest.

```{r, etude des résultats}
col=rainbow(4)
roc.curve(over_train$top_sanction,predict(model_list_over$ranger,x_over),col=col[1],main="ROC Curve avec Ranger")
roc.curve(y_test,predict(model_list_over$ranger,test_data),add.roc = T,col=col[2])
roc.curve(over_acp$top_sanction,predict(model_list_over_acp$ranger,x_over_acp),add.roc = T,col=col[3])
roc.curve(y_test,predict(model_list_over_acp$ranger,test_acp),add.roc = T,col=col[4])
legend('bottomright',legend=c("Train sans ACP","Test sans ACP","Train avec ACP","Test avec ACP"),lwd=2,col=col)
rm(col)
```

```{r, importance}
# #Importance des variables
# barplot(as.matrix(varImp(model_list_over$ranger)$importance)[,1],las=2,col=rainbow(ncol(over_train)),main="Importance des variables (Sans ACP)")
# 
# barplot(as.matrix(varImp(model_list_over_acp$ranger)$importance)[,1],las=2,col=rainbow(ncol(over_acp)),main="Importance des variables (Avec ACP)")
```

# XGBoost :

```{r}
Erreur(predict(model_list_over$xgbTree,x_over),over_train$top_sanction)
Erreur(predict(model_list_over$xgbTree,test_data),y_test)
Erreur(predict(model_list_over_acp$xgbTree,x_over_acp),over_acp$top_sanction)
Erreur(predict(model_list_over_acp$xgbTree,test_acp),y_test)
```

Sur-apprentissage, faibles résultats.

```{r, etude des résultats}
col=rainbow(4)
roc.curve(over_train$top_sanction,predict(model_list_over$xgbTree,x_over),col=col[1],main="ROC Curve XGBoost")
roc.curve(y_test,predict(model_list_over$xgbTree,test_data),add.roc = T,col=col[2])
roc.curve(over_acp$top_sanction,predict(model_list_over_acp$xgbTree,x_over_acp),add.roc = T,col=col[3])
roc.curve(y_test,predict(model_list_over_acp$xgbTree,test_acp),add.roc = T,col=col[4])
legend('bottomright',legend=c("Train sans ACP","Test sans ACP","Train avec ACP","Test avec ACP"),lwd=2,col=col)
rm(col)
```

```{r, importance}
#Importance des variables
barplot(as.matrix(varImp(model_list_over$xgbTree)$importance)[,1],las=2,col=rainbow(ncol(over_train)),main="Importance des variables (Sans ACP)")
barplot(as.matrix(varImp(model_list_over_acp$xgbTree)$importance)[,1],las=2,col=rainbow(ncol(over_acp)),main="Importance des variables (Avec ACP)")
```

# Comparaison des modèles entre eux : 

```{r, prediction sur le test}
print("Sans ACP : ")
print(model_names[1])
Erreur(predict(model_list_over$glm,test_data),y_test)
print(model_names[2])
Erreur(predict(model_list_over$knn,test_data),y_test)
print(model_names[3])
Erreur(predict(model_list_over$naive_bayes,test_data),y_test)
print(model_names[4])
Erreur(predict(model_list_over$svm_rad,test_data),y_test)
print(model_names[5])
Erreur(predict(model_list_over$svm_poly,test_data),y_test)
print(model_names[6])
Erreur(predict(model_list_over$rpart,test_data),y_test)
print(model_names[7])
Erreur(predict(model_list_over$ranger,test_data),y_test)
print(model_names[8])
Erreur(predict(model_list_over$rf,test_data),y_test)
print(model_names[9])
Erreur(predict(model_list_over$xgbTree,test_data),y_test)

col=rainbow(9)
roc.curve(y_test,predict(model_list_over$glm,test_data),col=col[1],main="ROC Curve")
roc.curve(y_test,predict(model_list_over$knn,test_data),col=col[2],add.roc = T)
roc.curve(y_test,predict(model_list_over$naive_bayes,test_data),col=col[3],add.roc = T)
roc.curve(y_test,predict(model_list_over$svm_rad,test_data),col=col[4],add.roc = T)
roc.curve(y_test,predict(model_list_over$svm_poly,test_data),col=col[5],add.roc = T)
roc.curve(y_test,predict(model_list_over$rpart,test_data),col=col[6],add.roc = T)
roc.curve(y_test,predict(model_list_over$ranger,test_data),col=col[7],add.roc = T)
roc.curve(y_test,predict(model_list_over$rf,test_data),col=col[8],add.roc = T)
roc.curve(y_test,predict(model_list_over$xgbTree,test_data),col=col[9],add.roc = T)
legend('bottomright',legend=model_names,col=col,lwd=2)

for(i in 1:5){print("")}

print("ACP : ")
print(model_names[1])
Erreur(predict(model_list_over_acp$glm,test_acp),y_test)
print(model_names[2])
Erreur(predict(model_list_over_acp$knn,test_acp),y_test)
print(model_names[3])
Erreur(predict(model_list_over_acp$naive_bayes,test_acp),y_test)
print(model_names[4])
Erreur(predict(model_list_over_acp$svm_rad,test_acp),y_test)
print(model_names[5])
Erreur(predict(model_list_over_acp$svm_poly,test_acp),y_test)
print(model_names[6])
Erreur(predict(model_list_over_acp$rpart,test_acp),y_test)
print(model_names[7])
Erreur(predict(model_list_over_acp$ranger,test_acp),y_test)
print(model_names[8])
Erreur(predict(model_list_over_acp$rf,test_acp),y_test)
print(model_names[9])
Erreur(predict(model_list_over_acp$xgbTree,test_acp),y_test)

roc.curve(y_test,predict(model_list_over_acp$glm,test_acp),col=col[1],main="ROC Curve avec ACP")
roc.curve(y_test,predict(model_list_over_acp$knn,test_acp),col=col[2],add.roc = T)
roc.curve(y_test,predict(model_list_over_acp$naive_bayes,test_acp),col=col[3],add.roc = T)
roc.curve(y_test,predict(model_list_over_acp$svm_rad,test_acp),col=col[4],add.roc = T)
roc.curve(y_test,predict(model_list_over_acp$svm_poly,test_acp),col=col[5],add.roc = T)
roc.curve(y_test,predict(model_list_over_acp$rpart,test_acp),col=col[6],add.roc = T)
roc.curve(y_test,predict(model_list_over_acp$ranger,test_acp),col=col[7],add.roc = T)
roc.curve(y_test,predict(model_list_over_acp$rf,test_acp),col=col[8],add.roc = T)
roc.curve(y_test,predict(model_list_over_acp$xgbTree,test_acp),col=col[9],add.roc = T)
legend('bottomright',legend=model_names,col=col,lwd=2)

rm(col,i)
```